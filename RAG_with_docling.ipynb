{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3edd566f-485c-496c-886d-75eb3810bac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping already-processed file (hash matched): 1728565027059.pdf\n",
      "‚ö†Ô∏è Skipping already-processed file (hash matched): Application_Agreement_.pdf\n",
      "‚ö†Ô∏è Skipping already-processed file (hash matched): aws-data-engineer-resume-example (1).pdf\n",
      "‚ö†Ô∏è Skipping already-processed file (hash matched): Bank Declaration Page 1.pdf\n",
      "‚ö†Ô∏è Skipping already-processed file (hash matched): Document.pdf\n",
      "‚ö†Ô∏è Skipping already-processed file (hash matched): K-5th - 2025 - GT Program Referral Flyer.pdf\n",
      "‚ö†Ô∏è Skipping already-processed file (hash matched): Leasing_Policy_.pdf\n",
      "‚ö†Ô∏è Skipping already-processed file (hash matched): Lesson Overview_LLM_GEN_AI.pdf\n",
      "‚ö†Ô∏è Skipping already-processed file (hash matched): LLM_2.pdf\n",
      "‚ö†Ô∏è Skipping already-processed file (hash matched): marriage_certificate.pdf\n",
      "‚ö†Ô∏è Skipping already-processed file (hash matched): USA1749351208.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üí¨ Enter your question:  what is data?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Context:\n",
      "Data Collection: Use large datasets from various sources, including internet scrapes and specialized corpora. Self-Supervised Learning: Model learns language patterns and structures. Compute Resources: Requires significant compute power, often GPUs. Data Quality: Only a small percentage (1-3%) of collected tokens are used after quality processing.\n",
      "\n",
      "Context:\n",
      "Data Collection: Use large datasets from various sources, including internet scrapes and specialized corpora. Self-Supervised Learning: Model learns language patterns and structures. Compute Resources: Requires significant compute power, often GPUs. Data Quality: Only a small percentage (1-3%) of collected tokens are used after quality processing.\n",
      "\n",
      "Context:\n",
      "Data Collection: Use large datasets from various sources, including internet scrapes and specialized corpora. Self-Supervised Learning: Model learns language patterns and structures. Compute Resources: Requires significant compute power, often GPUs. Data Quality: Only a small percentage (1-3%) of collected tokens are used after quality processing.\n",
      "\n",
      "Question: what is data?\n",
      "Helpful Answer: Data refers to information that is collected, stored, and processed by a computer system. In the context of natural language processing, data can include text, audio, and video recordings. The quality of the data is crucial to the success of the model, as it determines how accurately the model can learn and make predictions.\n",
      "\n",
      "üìÑ Source Documents:\n",
      "- Data Collection: Use large datasets from various sources, including internet scrapes and specialized corpora. Self-Supervised Learning: Model learns language patterns and structures. Compute Resources: Requires significant compute power, often GPUs. Data Quality: Only a small percentage (1-3%) of collected tokens are used after quality processing.\n",
      "- Data Collection: Use large datasets from various sources, including internet scrapes and specialized corpora. Self-Supervised Learning: Model learns language patterns and structures. Compute Resources: Requires significant compute power, often GPUs. Data Quality: Only a small percentage (1-3%) of collected tokens are used after quality processing.\n",
      "- Data Collection: Use large datasets from various sources, including internet scrapes and specialized corpora. Self-Supervised Learning: Model learns language patterns and structures. Compute Resources: Requires significant compute power, often GPUs. Data Quality: Only a small percentage (1-3%) of collected tokens are used after quality processing.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Final Working Example: Prevent Duplicate Processing Using File Hash\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import hashlib\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling.chunking import HybridChunker\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# üîá Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# üîê Set your Hugging Face API token\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"<<TOKEN>>\"\n",
    "\n",
    "# üìÇ Folder where PDFs are stored\n",
    "folder_path = \"source/\"\n",
    "\n",
    "# üìÅ Where to persist vectorstore\n",
    "persist_directory = \"./chroma_docling_db\"\n",
    "\n",
    "# üß† Initialize embedding and LLM models\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "llm = HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\", task=\"text-generation\", model_kwargs={\"temperature\": 0.3, \"max_new_tokens\": 512})\n",
    "\n",
    "# üîß Initialize Docling document converter\n",
    "converter = DocumentConverter()\n",
    "\n",
    "# üîß File hash function\n",
    "def get_file_hash(filepath):\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "# üß© Chunking logic\n",
    "def get_chunk_list(document):\n",
    "    chunker = HybridChunker(max_tokens=200, overlap_tokens=50)\n",
    "    return list(chunker.chunk(dl_doc=document))\n",
    "\n",
    "# üß± Convert Docling chunks to LangChain documents with file hash\n",
    "def safe_metadata(idx, chunk, file_hash):\n",
    "    heading = chunk.meta.headings[0] if chunk.meta.headings else \"\"\n",
    "    filename = getattr(chunk.meta.origin, \"filename\", \"unknown.pdf\")\n",
    "    page_numbers = str(\n",
    "        list({prov.page_no for item in chunk.meta.doc_items for prov in item.prov})\n",
    "    )\n",
    "    return {\n",
    "        \"chunk_id\": idx,\n",
    "        \"heading\": heading or \"\",\n",
    "        \"filename\": filename or \"unknown\",\n",
    "        \"file_hash\": file_hash,\n",
    "        \"page_numbers\": page_numbers\n",
    "    }\n",
    "\n",
    "# üíæ Store LangChain documents into Chroma vector DB\n",
    "def store_vector_db(documents):\n",
    "    vectorstore = Chroma.from_documents(documents, embedding_model, persist_directory=persist_directory)\n",
    "    vectorstore.persist()\n",
    "    print(\"‚úÖ Data stored successfully!\")\n",
    "\n",
    "# üîé Check if file is already processed by file_hash\n",
    "def is_file_already_stored(vectorstore, file_hash):\n",
    "    result = vectorstore._collection.get(where={\"file_hash\": file_hash})\n",
    "    return len(result[\"ids\"]) > 0\n",
    "\n",
    "# üîÅ Loop over PDFs and process if new\n",
    "vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        source = os.path.join(folder_path, filename)\n",
    "        file_hash = get_file_hash(source)\n",
    "\n",
    "        if is_file_already_stored(vectorstore, file_hash):\n",
    "            print(f\"‚ö†Ô∏è Skipping already-processed file (hash matched): {filename}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"üìÑ Processing: {filename}\")\n",
    "            result = converter.convert(source)\n",
    "            document = result.document\n",
    "            chunks = get_chunk_list(document)\n",
    "\n",
    "            long_chain_docs = [\n",
    "                Document(\n",
    "                    page_content=chunk.text,\n",
    "                    metadata=safe_metadata(idx, chunk, file_hash)\n",
    "                )\n",
    "                for idx, chunk in enumerate(chunks)\n",
    "            ]\n",
    "\n",
    "            store_vector_db(long_chain_docs)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to process {filename}: {e}\")\n",
    "\n",
    "# üß† Load vectorstore for retrieval\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# üîç Ask question interactively\n",
    "qa_chain = RetrievalQA.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "query = input(\"üí¨ Enter your question: \")\n",
    "result = qa_chain(query)\n",
    "\n",
    "print(\"\\nüß† Answer:\", result['result'])\n",
    "print(\"\\nüìÑ Source Documents:\")\n",
    "for doc in result['source_documents']:\n",
    "    print(\"-\", doc.page_content.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c6c1e-5727-4c1b-9d8c-8077760ae22b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
