# ğŸ“˜ Evaluating RAG Systems â€” Clear Notes

## ğŸ” Goal of the Lesson
This lesson explains **how to evaluate Retrieval-Augmented Generation (RAG) systems** using a principled framework called the **RAG Triad**, implemented with **TruLens**.

> ğŸ¯ **Goal:** Detect **hallucinations**, **retrieval failures**, and **answer quality issues** in RAG pipelines.

---

## ğŸ§± Core Concept: **RAG Triad**
The **RAG Triad** evaluates the **three key stages** of a RAG system:

1. **Retrieval**
2. **Grounding**
3. **Generation**

### âœ… The Three Metrics
- **Context Relevance**
- **Groundedness**
- **Answer Relevance**

These metrics are implemented as **feedback functions** in **TruLens**.

---

## ğŸ› ï¸ Tools & Setup

### ğŸ”‘ Prerequisites
- **TruLens-eval**
- **LlamaIndex**
- **OpenAI API key**

### ğŸ§° Key Components
- **Tru object (`Tru`)** â€“ manages the evaluation database
- Records **prompts, responses, intermediate steps, and scores**

---

## ğŸ“„ Document Processing & Retrieval

### ğŸ“š Data Source
- PDF: *Building a Career in AI* by **Andrew Ng**

### ğŸ”§ Indexing Setup
- Single merged document
- **Sentence Index**
- **bge-small-v1.5 embedding model**
- **GPT-3.5 Turbo** (temperature = 0.1)

### ğŸ” Retrieval Engine
- **Sentence Window Query Engine**

---

## â“ Example Query
**â€œHow do you create your AI portfolio?â€**

The system returns:
- Final answer
- Retrieved context
- Metadata

Evaluation is required to verify **trustworthiness**.

---

## ğŸ”º The RAG Triad â€” Detailed Breakdown

### 1ï¸âƒ£ **Answer Relevance**
Checks whether the **final answer** addresses the **user query**.
- Score: **0â€“1**
- May include **supporting reasoning**

**Failure Mode:** Answer is true but unrelated.

---

### 2ï¸âƒ£ **Context Relevance**
Evaluates whether retrieved documents are relevant to the query.
- Scores each retrieved chunk
- Computes a **mean relevance score**

**Failure Mode:** Retrieval pulls irrelevant context.

---

### 3ï¸âƒ£ **Groundedness**
Checks whether the answer is **supported by retrieved context**.
- Sentence-level scoring
- Scores averaged

**Failure Mode (Hallucination):** Answer uses model knowledge not present in context.

---

## ğŸ” Evaluation & Iteration Workflow
1. Start with basic RAG
2. Evaluate using **RAG Triad**
3. Identify failure modes
4. Improve retrieval
5. Re-evaluate

---

## ğŸªŸ Sentence Window RAG Tuning
- **Small window** â†’ insufficient context
- **Large window** â†’ irrelevant information

Goal: **Balanced window size**

---

## ğŸ§ª Feedback Function Types

### ğŸ¤– LLM-Based
- GPT-3.5 / GPT-4
- Semantic and scalable

### ğŸ‘¤ Human Evaluation
- ~80% agreement with LLM judges

### ğŸ“Š Traditional NLP Metrics
- ROUGE, BLEU (syntactic, limited for RAG)

---

## ğŸ—ƒï¸ Recording & Results

### ğŸ§¾ TruRecorder
- Logs inputs, outputs, scores, latency, and cost
- Stored in **JSON format**

### ğŸ“Š Streamlit Dashboard
- Leaderboard view
- Record-level inspection

**Example Insight:**
Low groundedness when statements lack supporting retrieved context.

---

## ğŸ§  Open-Book Exam Analogy
- **Context Relevance:** Opened the right page
- **Groundedness:** Used information from that page
- **Answer Relevance:** Answered the question asked

---

## ğŸ“ Key Takeaways
- **RAG Triad = Context Relevance + Groundedness + Answer Relevance**
- **Groundedness** is key to detecting hallucinations
- Evaluation is **iterative**
- Sentence-window RAG improves grounding
