{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf16137-6170-483c-b416-69b318164837",
   "metadata": {},
   "source": [
    "#### LLM chaining\n",
    "  üîπ LLMChain\n",
    "  \n",
    "        SequentialChain\n",
    "        RouterChain\n",
    "        TransformChain\n",
    "        RetrievalChain (v0.1) ‚úîÔ∏è Correct ‚Äî now part of retrieval module\n",
    "    \n",
    "  üîπ NEW in LangChain 0.1+\n",
    "  \n",
    "        Runnable sequences\n",
    "        Runnable mapping\n",
    "        Parallel chains\n",
    "        Streaming pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c83de-6ff5-4d3f-aa97-9cfaa32522b7",
   "metadata": {},
   "source": [
    "Chains = connecting multiple components into a pipeline:\n",
    "\n",
    "Prompt ‚Üí LLM ‚Üí Output Parser\n",
    "\n",
    "Loader ‚Üí Splitter ‚Üí VectorStore ‚Üí Retriever ‚Üí LLM\n",
    "\n",
    "Decision routing\n",
    "\n",
    "Multi-step logic\n",
    "\n",
    "Below are examples of ALL important chain types:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b969fc7b-6b83-4daf-af36-86019cc6767b",
   "metadata": {},
   "source": [
    "#### LLMChain ‚Äî Basic Prompt ‚Üí LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "414f9fdc-09b3-414d-a8ec-114fa9f29c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c707cd-00d4-4696-933b-effc5d6cdb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o-mini\",\n",
    "                 api_key = api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da64f3a8-aae3-4237-8fd2-d34fa40a2475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in one paragraph.\",\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "print(chain.invoke({\"topic\": \"feature engineering\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dfba90-6338-453b-996b-179c7e789dbc",
   "metadata": {},
   "source": [
    "### ‚úÖ 3.2 SequentialChain ‚Äî Multi-step pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932804f4-744f-401c-83c8-fd69eab1d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Step 1: Generate ideas\n",
    "idea_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Generate 3 creative ideas about {topic}.\"\n",
    ")\n",
    "\n",
    "# Step 2: Expand the best idea\n",
    "expand_prompt = PromptTemplate(\n",
    "    input_variables=[\"ideas\"],\n",
    "    template=\"Pick the best idea from this list and expand it into a paragraph:\\n{ideas}\"\n",
    ")\n",
    "\n",
    "# --- Build the pipeline ---\n",
    "\n",
    "# Step 1 ‚Üí produce \"ideas\"\n",
    "step1 = idea_prompt | llm | parser\n",
    "\n",
    "# Step 2 ‚Üí consume \"ideas\" ‚Üí produce final output\n",
    "step2 = expand_prompt | llm | parser\n",
    "\n",
    "# Complete chain\n",
    "chain = (\n",
    "    {\"ideas\": step1}  # Run step1 and store output in \"ideas\"\n",
    "    | step2           # Pass \"ideas\" into step2\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"topic\": \"AI in healthcare\"})\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8256e25-5e26-4a83-b6be-fde5e8d627e2",
   "metadata": {},
   "source": [
    "### ‚≠ê 3.3 RouterChain ‚Äî Choose the right chain based on input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ae043-904f-434d-b74a-283610f79cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Destination prompts\n",
    "math_prompt = PromptTemplate.from_template(\"Solve this math problem: {input}\")\n",
    "joke_prompt = PromptTemplate.from_template(\"Tell a joke about: {input}\")\n",
    "define_prompt = PromptTemplate.from_template(\"Provide a definition of: {input}\")\n",
    "\n",
    "# Default fallback prompt\n",
    "default_prompt = PromptTemplate.from_template(\"Answer this question normally: {input}\")\n",
    "\n",
    "# Router logic (replacement for MultiPromptChain)\n",
    "router = RunnableBranch(\n",
    "    # If input looks like math\n",
    "    (lambda x: any(c.isdigit() for c in x[\"input\"]), math_prompt | llm | parser),\n",
    "\n",
    "    # If input mentions \"joke\"\n",
    "    (lambda x: \"joke\" in x[\"input\"].lower() or \"funny\" in x[\"input\"].lower(),\n",
    "        joke_prompt | llm | parser),\n",
    "\n",
    "    # If input asks for definition\n",
    "    (lambda x: \"define\" in x[\"input\"].lower() or \"what is\" in x[\"input\"].lower(),\n",
    "        define_prompt | llm | parser),\n",
    "\n",
    "    # Default fallback\n",
    "    default_prompt | llm | parser\n",
    ")\n",
    "\n",
    "# Test\n",
    "print(router.invoke({\"input\": \"5 + 7\"}))\n",
    "print(router.invoke({\"input\": \"Tell me something funny\"}))\n",
    "print(router.invoke({\"input\": \"Explain transformers\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b431743b-5f72-405b-b24a-f60a6d654862",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ‚≠ê TransformChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a99085-1abe-4568-b488-5e1063b2f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Step 1: Clean the text\n",
    "clean_step = RunnableLambda(lambda x: {\"text\": x[\"text\"].strip()})\n",
    "\n",
    "# Step 2: Format into a prompt (REQUIRED!)\n",
    "prompt = PromptTemplate.from_template(\"Answer this: {text}\")\n",
    "\n",
    "# Step 3: Final chain\n",
    "chain = clean_step | prompt | llm | parser\n",
    "\n",
    "print(chain.invoke({\"text\": \"   Explain feature engineering   \"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ddb29-436f-4bae-b9b8-c3a2698c079f",
   "metadata": {},
   "source": [
    "### üîµ 2. RetrievalChain ‚Äî Auto RAG Pipeline\n",
    "\n",
    "        A RetrievalChain combines:\n",
    "\n",
    "            Retriever\n",
    "\n",
    "            Prompt\n",
    "\n",
    "            LLM\n",
    "\n",
    "            Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a820936f-7792-4792-9860-992ba595cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U langchain langchain-community langchain-core langchain-openai langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdbac585-0333-49cc-a11e-c785574ac753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.2\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d25c4f6-5540-406a-98c9-de972e79dbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbb940b6-9309-473a-a328-7c44440ada38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='FAISS is used for fast vector similarity search.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 163, 'total_tokens': 173, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_6318584bd8', 'id': 'chatcmpl-CkfIYaiQR18RgEmmQSVUfM6KDKDp4', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b0060-574a-7bb3-be9f-1b29458deb5e-0' usage_metadata={'input_tokens': 163, 'output_tokens': 10, 'total_tokens': 173, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --------------------------\n",
    "# Sample documents\n",
    "# --------------------------\n",
    "docs = [\n",
    "    \"LangChain is a framework for building LLM applications.\",\n",
    "    \"FAISS is used for fast vector similarity search.\",\n",
    "    \"RAG improves LLM accuracy using document retrieval.\"\n",
    "]\n",
    "\n",
    "# --------------------------\n",
    "# Split documents\n",
    "# --------------------------\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
    "documents = splitter.create_documents(docs)\n",
    "\n",
    "# --------------------------\n",
    "# Create vector store + retriever\n",
    "# --------------------------\n",
    "embeddings = OpenAIEmbeddings(api_key=api_key)\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# --------------------------\n",
    "# Build the prompt template\n",
    "# --------------------------\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the question using ONLY the context.\"),\n",
    "    (\"human\", \"Context:\\n{context}\\n\\nQuestion:\\n{question}\")\n",
    "])\n",
    "\n",
    "# --------------------------\n",
    "# LLM\n",
    "# --------------------------\n",
    "llm = ChatOpenAI(model = \"gpt-4o-mini\",\n",
    "                 api_key = api_key)\n",
    "\n",
    "# --------------------------\n",
    "# Build RAG pipeline (LCEL)\n",
    "# --------------------------\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Ask a question\n",
    "# --------------------------\n",
    "response = rag_chain.invoke(\"What is FAISS used for?\")\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18c28ce-db4e-42bc-9600-822cf23882c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b4260-6044-484f-a002-0dfbbc6adc58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
