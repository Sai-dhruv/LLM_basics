{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ba5023-a3d8-4fa1-b634-a13dda6dc3b4",
   "metadata": {},
   "source": [
    "## üß† Context Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a655b033-db3c-45b7-965c-b8f6cb31e618",
   "metadata": {},
   "source": [
    "##### ‚ÄúContext Engineering‚Äù is all about giving the model the right knowledge at the right time so its output is accurate, relevant, and useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e2ad3f-59bf-4808-8b6c-2d3275384bbb",
   "metadata": {},
   "source": [
    "### üîé 1. RAG Pipeline (Retrieval-Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f81cdd2-5843-42e8-a2d4-ff9154bb33b6",
   "metadata": {},
   "source": [
    "üìå What it is:<br>\n",
    "RAG = Retrieval-Augmented Generation ‚Äî a technique where instead of depending only on the LLM‚Äôs internal knowledge, we retrieve relevant external information (documents, database entries, APIs) and  inject it into the prompt before generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df91c784-cd58-44ac-87f2-ac04c295bf95",
   "metadata": {},
   "source": [
    "### ‚úÖ Key Concepts & Keywords:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c346bce6-ffe2-4f3c-9f1e-f63bc24e8309",
   "metadata": {},
   "source": [
    "| Concept                     | Explanation                                                  | Keywords                               |\n",
    "| --------------------------- | ------------------------------------------------------------ | -------------------------------------- |\n",
    "| **Retrieval**               | Searching for relevant chunks of data based on the query     | Similarity search, top-k retrieval     |\n",
    "| **Embedding**               | Converting text into numeric vectors                         | Sentence embeddings, cosine similarity |\n",
    "| **Chunking**                | Splitting documents into smaller pieces for better retrieval | Sliding window, fixed-size chunks      |\n",
    "| **Vector Database**         | Stores embeddings and retrieves similar ones                 | Pinecone, FAISS, Chroma, Weaviate      |\n",
    "| **Augmentation**            | Adding retrieved context into the prompt                     | Context injection                      |\n",
    "| **Prompt Construction**     | Combine user query + retrieved info                          | Template + context block               |\n",
    "| **Grounding**               | Ensuring answers are based on real data                      | Evidence-based generation              |\n",
    "| **Hallucination reduction** | Prevent model from guessing facts                            | Fact grounding, context constraints    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf325037-905f-4a9c-888b-b6dc97442e96",
   "metadata": {},
   "source": [
    "### üß† 2. Memory (Short-term & Long-term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a807b54-6939-499b-95c5-780edd5e5483",
   "metadata": {},
   "source": [
    "üìå What it is: <br>\n",
    "Memory allows an LLM to remember past interactions so responses are consistent and context-aware. It‚Äôs crucial for multi-turn conversations, personal assistants, and chatbots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd3b1f-b298-4e15-99b7-1eedab85d6dc",
   "metadata": {},
   "source": [
    "## ‚úÖ Types of Memory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d86e451-3e21-4ff6-ac42-f7c05c05a518",
   "metadata": {},
   "source": [
    "| Type                  | Purpose                                                  | Keywords                        |\n",
    "| --------------------- | -------------------------------------------------------- | ------------------------------- |\n",
    "| **Short-term memory** | Stores previous conversation turns within context window | Conversation buffer, history    |\n",
    "| **Summarized memory** | Compress older conversations into summaries              | Rolling summary                 |\n",
    "| **Long-term memory**  | Persistent storage of user data or interactions          | Vector memory, retrieval memory |\n",
    "| **Session memory**    | Context limited to a session                             | Token limit, conversation state |\n",
    "| **Context window**    | Max tokens the model can remember                        | 8k, 16k, 128k context           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7986f61a-efb7-4e81-a60b-3c40195c7d10",
   "metadata": {},
   "source": [
    "### ‚úÖ Example ‚Äì Chatbot Memory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a50f2dd-26ff-4b6b-84a8-40b6a4865903",
   "metadata": {},
   "source": [
    "##### Without memory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6307e027-d7f8-4d97-bf1b-06fde2dd4686",
   "metadata": {},
   "source": [
    "User: What's the capital of France? <br>\n",
    "Bot: Paris <br>\n",
    "User: What's its population? <br>\n",
    "Bot: (Doesn't remember \"France\") ‚Üí Might fail. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fdd613-c30c-405c-adcc-516845e0f409",
   "metadata": {},
   "source": [
    "#### With memory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44ac159-11cb-4b62-b862-6b813742f244",
   "metadata": {},
   "source": [
    "Bot remembers previous Q&A: <br>\n",
    "\"France ‚Üí capital = Paris\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f69485-84e9-4cf8-b179-fa8c87b30599",
   "metadata": {},
   "source": [
    "‚úÖ Real-world Use Case:\n",
    "\n",
    "Customer support bots: remember ticket ID across multiple messages.\n",
    "\n",
    "Virtual assistants: remember user preferences (e.g., vegetarian meals).\n",
    "\n",
    "Agent workflows: pass previous results into next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8733f7be-4c86-4a38-b372-81d678f808af",
   "metadata": {},
   "source": [
    "### üóÇÔ∏è 3. Vector Database (Knowledge Store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b563f5f-97a8-4b61-a16c-cdd9f6b25573",
   "metadata": {},
   "source": [
    "üìå What it is: <br>\n",
    "A vector database stores and retrieves embeddings ‚Äî numerical representations of text. It‚Äôs the ‚Äúmemory brain‚Äù behind RAG pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31e388-a2c4-486a-ba01-b473ef1bdfd9",
   "metadata": {},
   "source": [
    "| Concept                      | Description                                           |\n",
    "| ---------------------------- | ----------------------------------------------------- |\n",
    "| **Embedding**                | Numeric representation of semantic meaning            |\n",
    "| **Vector similarity search** | Find documents closest to query vector                |\n",
    "| **Indexing**                 | Organizing vectors for fast retrieval (HNSW, IVF, PQ) |\n",
    "| **Top-k search**             | Retrieve top N most similar results                   |\n",
    "| **Metadata filtering**       | Search with filters (e.g., author, date)              |\n",
    "| **Hybrid search**            | Combine keyword + vector search                       |\n",
    "| **Vector DB examples**       | Pinecone, Chroma, Weaviate, FAISS                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661fea1a-f823-482a-bb5f-8d2717f6dc95",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è 4. Dynamic Context Injection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac830187-7413-43cf-9be5-45113167c8a7",
   "metadata": {},
   "source": [
    "üìå What it is: <br>\n",
    "Injecting only the most relevant context into the LLM prompt at runtime based on the user query ‚Äî instead of loading everything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45fa70b-204c-471f-ade5-c9610c1ade95",
   "metadata": {},
   "source": [
    "#### ‚úÖ Concepts & Keywords:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d846b50-0fc4-476d-b1f2-b18877886201",
   "metadata": {},
   "source": [
    "| Concept                 | Meaning                                  |\n",
    "| ----------------------- | ---------------------------------------- |\n",
    "| **Dynamic retrieval**   | Fetching context based on query intent   |\n",
    "| **Context selection**   | Choosing top relevant docs               |\n",
    "| **Context ranking**     | Prioritizing info by relevance           |\n",
    "| **Prompt construction** | Assembling dynamic template with context |\n",
    "| **Context truncation**  | Keeping prompt within token limit        |\n",
    "| **Context compression** | Summarizing large retrieved chunks       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beebf0e-1873-4ded-bef3-e1550182b93f",
   "metadata": {},
   "source": [
    "‚úÖ Example:\n",
    "\n",
    "Query: ‚ÄúHow does LangGraph manage state?‚Äù\n",
    "\n",
    "Instead of injecting all docs, we dynamically retrieve only the relevant ones:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1385869a-e1dd-468a-8468-b39f797a50f3",
   "metadata": {},
   "source": [
    "Context:\n",
    "- LangGraph uses stateful nodes to pass information between agents.\n",
    "- State includes memory, tool outputs, and messages.\n",
    "\n",
    "Answer the question based on the context above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e68fa45-2c6e-4817-a379-584a5132732c",
   "metadata": {},
   "source": [
    "üì§ Output: <br>\n",
    "‚ÄúLangGraph manages state by maintaining structured data between agent nodes‚Ä¶‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9b2c01-dc23-4fac-8129-2193027ec547",
   "metadata": {},
   "source": [
    "‚úÖ Real-World Use:\n",
    "\n",
    "Chatbots: only load context relevant to current question.\n",
    "\n",
    "Search-based RAG: dynamically fetch docs for every query.\n",
    "\n",
    "Large KBs: avoid hitting token limits by loading only what‚Äôs needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb2dac-e1a3-4c1c-b7f1-0b75b8cd2367",
   "metadata": {},
   "source": [
    "### üë§ 5. Personalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c06a13-e766-43d3-b2e9-2e983000fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "üìå What it is:\n",
    "Tailoring the model‚Äôs responses based on user identity, preferences, history, or behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8342a0-6d19-4eb2-9bb9-d1c8f880a68f",
   "metadata": {},
   "source": [
    "‚úÖ Concepts & Keywords:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939de457-d7cc-4137-98df-3b32a116cd3e",
   "metadata": {},
   "source": [
    "| Concept                          | Description                                 |\n",
    "| -------------------------------- | ------------------------------------------- |\n",
    "| **User profile**                 | Basic data: name, preferences, location     |\n",
    "| **Behavioral context**           | Past choices, click patterns, usage history |\n",
    "| **Adaptive prompting**           | Adjusting tone/style based on user          |\n",
    "| **Custom context injection**     | Adding user-specific knowledge into prompt  |\n",
    "| **Memory-based personalization** | Recall past chats and customize replies     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783585bd-5dfd-4a36-9afe-15f7be3d7230",
   "metadata": {},
   "source": [
    "‚úÖ Example ‚Äì Personalized Prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad71c0a0-ef7f-409c-b426-7c27e1d91106",
   "metadata": {},
   "source": [
    "System: You are a travel assistant.\n",
    "\n",
    "User profile:\n",
    "- Name: Alex\n",
    "- Preferred style: Casual\n",
    "- Travel preference: Budget-friendly, vegetarian food\n",
    "\n",
    "Prompt:\n",
    "\"Plan a 5-day trip to Tokyo.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3314cbcc-4003-48a3-9a5b-280a18456b6e",
   "metadata": {},
   "source": [
    "üì§ Output: <br>\n",
    "‚ÄúHey Alex! Here‚Äôs a fun, budget-friendly itinerary for 5 days in Tokyo üç£ (with plenty of vegetarian spots)...‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17fc83c-5204-4bc5-aed4-9e3f8c1ab0c0",
   "metadata": {},
   "source": [
    "‚úÖ Real-World Use Cases:\n",
    "\n",
    "Personalized tutors (adapt difficulty level based on history)\n",
    "\n",
    "Shopping assistants (recommend products based on past purchases)\n",
    "\n",
    "Banking chatbots (remember account type and preferences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f025f89-1114-43e2-8be8-94b1900cfd6a",
   "metadata": {},
   "source": [
    "### üß† Quick Summary Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8486099e-11f7-46c0-b75f-02f4fef90ae0",
   "metadata": {},
   "source": [
    "| Concept               | What It Does                           | Tools/Tech                     | Example                                               |\n",
    "| --------------------- | -------------------------------------- | ------------------------------ | ----------------------------------------------------- |\n",
    "| **RAG**               | Inject external knowledge dynamically  | LangChain, LlamaIndex          | Retrieve docs ‚Üí inject into prompt                    |\n",
    "| **Memory**            | Remember context across turns/sessions | Conversation buffer, Redis, DB | Chatbot remembers previous question                   |\n",
    "| **Vector DB**         | Store and search embeddings            | Pinecone, Chroma, Weaviate     | Retrieve top-k relevant docs                          |\n",
    "| **Dynamic Injection** | Load only relevant context at runtime  | LangChain retrievers           | Inject specific paragraphs                            |\n",
    "| **Personalization**   | Customize output based on user         | User profile, metadata         | ‚ÄúAlex prefers vegetarian ‚Üí recommend veg restaurants‚Äù |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94678ec-e3f6-4031-8f3c-eaef4801c0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
