{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7277a46f-63b2-4f4c-8826-b80a42f8e8b1",
   "metadata": {},
   "source": [
    "‚úÖ Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86dc6ee-23bd-4d74-84d5-dae2b74dbe04",
   "metadata": {},
   "source": [
    "### Copics covered in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a102a-429c-426d-ba2a-94964ee52b45",
   "metadata": {},
   "source": [
    "‚úÖZero-shot<br> ‚úÖfew-shot<br> ‚úÖCoT<br> ‚úÖReAct<br> ‚úÖtool calling<br> ‚úÖoutput formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4df6ad-f090-4806-84ef-05e8ffc65ce0",
   "metadata": {},
   "source": [
    "## üß† 1. Prompt Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632b3c6a-ea11-4ccd-8ab3-a85e5bd68284",
   "metadata": {},
   "source": [
    "‚úÖZero-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b120d546-2bc5-4ed7-85c4-8b699464d1c4",
   "metadata": {},
   "source": [
    "Ask directly with no examples <br>\n",
    "‚ÄúTranslate this sentence into French: How are you?‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217d7617-c6f4-420a-a660-af8fe1fa1deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "‚úÖ One-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b9e30-86d4-48eb-ae9d-0d7bc7dfb412",
   "metadata": {},
   "source": [
    "Give one example to guide the model<br>\n",
    "‚ÄúExample: Dog ‚Üí Perro. Now translate Cat ‚Üí‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ebfbd4-2b98-4093-8862-b3b7514b21ef",
   "metadata": {},
   "source": [
    "‚úÖ few-shot :\n",
    "Concept:<br>\n",
    "Giving the model a few good demonstrations improves accuracy and consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9aea5e-9821-46f0-b6ec-3896b014949c",
   "metadata": {},
   "source": [
    "Give multiple examples for better accuracy <br>\n",
    "‚ÄúApple ‚Üí Manzana\n",
    "Dog ‚Üí Perro\n",
    "Cat ‚Üí‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744aa0ad-7010-4c2a-b32f-a6c925392339",
   "metadata": {},
   "source": [
    "## üß± 2. Instruction Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8de8b4-84dc-435f-8437-89f7053499e0",
   "metadata": {},
   "source": [
    "Concept:\n",
    "It‚Äôs about how clearly and precisely you tell the model what you want.\n",
    "You must specify:\n",
    "\n",
    "Task (what to do)\n",
    "\n",
    "Constraints (length, format, style)\n",
    "\n",
    "Output format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ce5322-0835-402b-9620-17ceca26e32f",
   "metadata": {},
   "source": [
    "‚úÖ Example:\n",
    "\n",
    "‚ùå Vague Prompt:\n",
    "‚ÄúWrite about climate change.‚Äù\n",
    "\n",
    "‚úÖ Good Instruction:\n",
    "‚ÄúWrite a 100-word summary about climate change‚Äôs impact on agriculture. Use formal tone and bullet points.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849c45b7-a0f3-4bed-a517-f1c0558dda4b",
   "metadata": {},
   "source": [
    "## üßë‚Äç‚öñÔ∏è 3. System vs. User vs. Assistant Roles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb1dddc-69d5-4afb-8ea2-a2daf8222912",
   "metadata": {},
   "source": [
    "Concept:\n",
    "LLMs (like OpenAI, Anthropic) follow a message hierarchy:\n",
    "\n",
    "Role\t :  Purpose<br>\n",
    "System\t :  Global instruction (highest priority)<br>\n",
    "User\t :  What the user wants<br>\n",
    "Assistant:\tModel‚Äôs previous messages (context)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ab997d5-1210-484f-aa77-66b69ee42d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a strict grammar corrector.\"},\n",
    "  {\"role\": \"user\", \"content\": \"i going to school yesterday.\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6a0365-a0e5-4bb6-a141-cad1b745fe4e",
   "metadata": {},
   "source": [
    "## üß∞ 4. Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02307f35-766a-4168-a1ac-e31dcb90a829",
   "metadata": {},
   "source": [
    "Concept:<br>\n",
    "You don‚Äôt hardcode prompts ‚Äî you parameterize them (like variables) to reuse across tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c7261b6-a0c9-47e6-b97a-67fc72b42860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are a helpful assistant.\\nSummarize the following text in 50 words:\\n\"Artificial Intelligence is transforming...\"\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "You are a helpful assistant.\n",
    "Summarize the following text in {word_limit} words:\n",
    "\"{text}\"\n",
    "\"\"\"\n",
    "\n",
    "prompt = template.format(word_limit=50, text=\"Artificial Intelligence is transforming...\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dc8c83-eb48-4c86-8a69-d4937aa8cd14",
   "metadata": {},
   "source": [
    "## üì¶ 5. Output Structuring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa926e5-a15e-4054-b428-b8176307337b",
   "metadata": {},
   "source": [
    "Concept:<br>\n",
    "Control the output format so your application can parse it (e.g., JSON).<br>\n",
    "This is critical in production ‚Äî otherwise, LLMs return free text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6bdee6ea-c5bf-4853-97de-63cfa5244569",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (445795130.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[35], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    Output format:\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#You are an API that returns structured JSON.\n",
    "\n",
    "Input: \"Book: Atomic Habits by James Clear\"\n",
    "Output format:\n",
    "{\n",
    "  \"title\": \"<title>\",\n",
    "  \"author\": \"<author>\",\n",
    "  \"genre\": \"<genre>\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6a081e-168a-4477-b4f0-e55459fbce6f",
   "metadata": {},
   "source": [
    "## üîó 7. Chain-of-Thought (CoT) Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd39b55-13a3-4147-993d-e4051149b09a",
   "metadata": {},
   "source": [
    "Concept:<br>\n",
    "Encourage the model to show its reasoning step-by-step before answering. This often boosts accuracy for logic/math tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b143f33b-2744-4e97-9661-c55975034304",
   "metadata": {},
   "source": [
    "Solve this step-by-step:<br>\n",
    "If a train travels 60 km in 1.5 hours, what is its average speed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b113025e-266c-4ebc-b9f9-2f5a2c3e8c64",
   "metadata": {},
   "source": [
    "## üß† 8. Self-Refine / ReAct / Tool-Calling Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee134b4c-d8b4-4c90-bcdc-fffcb87f1cc9",
   "metadata": {},
   "source": [
    "Concept:<br>\n",
    "These are advanced techniques where the model plans, acts, and reflects or calls tools to solve tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21500b90-0695-42aa-92de-a731b61acf99",
   "metadata": {},
   "source": [
    "| Type             | Meaning                                         | Example Use              |\n",
    "| ---------------- | ----------------------------------------------- | ------------------------ |\n",
    "| **Self-Refine**  | Model critiques its own answer and improves it  | Text refinement, QA      |\n",
    "| **ReAct**        | Reason + Act cycle (think ‚Üí call tool ‚Üí update) | Web search, calculations |\n",
    "| **Tool-calling** | Model invokes external API/function             | Weather API, DB query    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c22d82e-c171-44ef-9e52-c05339cb3d08",
   "metadata": {},
   "source": [
    "### ‚úÖ ReAct Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821d38dd-f8ce-48a8-bbe3-957d6f20cb55",
   "metadata": {},
   "source": [
    "Question: What is the population of France?\n",
    "\n",
    "Thought: I don‚Äôt know directly. I should look it up. <br>\n",
    "Action: Search(\"population of France\") <br>\n",
    "Observation: 67 million (2024) <br>\n",
    "Answer: France has a population of about 67 million."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c274a233-1b44-4308-a956-a724a0c72215",
   "metadata": {},
   "source": [
    "## üìä 9. Prompt Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f44a1d3-52f8-4627-8b42-25a4f52abaab",
   "metadata": {},
   "source": [
    "Concept:<br>\n",
    "You must measure how well prompts perform. Some common metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e6eb8-55a8-4bd4-abc3-740d6381ee6c",
   "metadata": {},
   "source": [
    "| Metric                 | Meaning                          | When to Use          |\n",
    "| ---------------------- | -------------------------------- | -------------------- |\n",
    "| **Precision**          | % of correct positive outputs    | Classification tasks |\n",
    "| **Factuality**         | % of responses factually correct | Knowledge Q&A        |\n",
    "| **Hallucination Rate** | % of outputs with false info     | RAG / chatbot        |\n",
    "| **BLEU / ROUGE**       | Text similarity with reference   | Summarization        |\n",
    "| **Latency / Cost**     | Time & tokens consumed           | Optimization         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb3efcc-6db5-41e2-97e6-fe0ca73cb8b1",
   "metadata": {},
   "source": [
    "Example Evaluation:\n",
    "\n",
    "Task: Summarize 100 docs\n",
    "\n",
    "Output correct on 93 ‚Üí Precision = 93%\n",
    "\n",
    "5 responses hallucinated ‚Üí Hallucination Rate = 5%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe8a163-e89d-427f-b9d3-dbf5753626ea",
   "metadata": {},
   "source": [
    "## üìö Quick Recap Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f82b51-4463-45cf-a35c-8b661e7dfc11",
   "metadata": {},
   "source": [
    "| Concept                 | What it Does              | Example                      |\n",
    "| ----------------------- | ------------------------- | ---------------------------- |\n",
    "| **Zero-shot**           | Direct task               | ‚ÄúTranslate to French: Hello‚Äù |\n",
    "| **Few-shot**            | Uses examples             | ‚ÄúApple‚ÜíManzana, Dog‚ÜíPerro‚Ä¶‚Äù  |\n",
    "| **Instruction Design**  | Precise task definition   | ‚ÄúSummarize in 50 words‚Äù      |\n",
    "| **System vs User**      | Control behavior globally | System: ‚ÄúBe formal.‚Äù         |\n",
    "| **Prompt Templates**    | Reusable & dynamic        | `{word_limit} words summary` |\n",
    "| **Output Structuring**  | Force JSON/format         | Return structured data       |\n",
    "| **Chain-of-Thought**    | Step-by-step reasoning    | Solve math problem           |\n",
    "| **ReAct / Self-Refine** | Plan‚ÄìAct‚ÄìReflect          | Use API during reasoning     |\n",
    "| **Prompt Evaluation**   | Measure quality           | Factuality, BLEU, latency    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927c4e29-3e1e-46b9-8aab-d1d3c40a1343",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
