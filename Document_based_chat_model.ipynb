{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4266832-23fc-4e1d-afba-1facb0b27e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import hashlib\n",
    "import os\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6a6ecb3-053c-4cbc-9be9-dff2813a9c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PINECONE_API_KEY\"] = <<PINECONE_API_KEY>>\n",
    "os.environ[\"COHERE_API_KEY\"] = <<COHERE_API_KEY>>\n",
    "os.environ[\"OPENAI_API_KEY\"] = <<OPENAI_API_KEY>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0c276cc9-77f1-4110-82a4-60ca1df2fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define index configuration\n",
    "INDEX_NAME = 'student'\n",
    "file_type = 'txt'\n",
    "filename = \"USA.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "95d2dc66-de89-4fc2-8ac1-6970a325f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3191ae-868e-4a35-b0ef-cc8dd80008b9",
   "metadata": {},
   "source": [
    "#### Load and text split using langchain frame work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a02859-6c00-4832-a7f4-864a659f5758",
   "metadata": {},
   "source": [
    "##### <b><font color=orange>The langchain.document_loaders module in the LangChain library provides a variety of tools to load documents from different sources and formats.</font></b>\n",
    "    Common Document Loaders :\n",
    "        PyPDFLoader\n",
    "        TextLoader\n",
    "        CSVLoader\n",
    "        JSONLoader etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a31190b6-c513-4d35-a8c1-1a5f3c81cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c7a9d16b-1722-4ac0-abcd-e876b9dac10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a3109702-9cab-4858-b440-67364295644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(filename,file_type):\n",
    "    try:\n",
    "        if file_type == 'txt':\n",
    "            loader = TextLoader(filename)\n",
    "        else:\n",
    "            loader = PyPDFLoader(filename)\n",
    "        return loader\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename}\")\n",
    "        return []\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0b629f-ccea-42f2-b4f3-80d1b4a58c42",
   "metadata": {},
   "source": [
    "#### <center><b><font color=pink>Choosing the right text splitter depends on the content and use case:</font></b></center>\n",
    "<b>RecursiveCharacterTextSplitter:</b> Best for creating chunks that resemble natural language breaks, which is useful for question-answering systems.<br>\n",
    "<b>CharacterTextSplitter:</b> Simple and works when the text can be split roughly without specific formatting.<br>\n",
    "<b>TokenTextSplitter:</b> Ideal when working with models that have a token limit, ensuring each chunk is optimized for token-based language models.<br>\n",
    "<b>MarkdownTextSplitter:</b> Useful for documents written in Markdown format, respecting headers, lists, and code blocks.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed8d8e9-8a3d-4107-b290-6683fdd8c422",
   "metadata": {},
   "source": [
    "#### <font color = 'red'>**TIP**</font>:\n",
    "**<font color = 'blue'>Chunks of about to 500 to 1000 characters with a 10 - 20 % \n",
    "overlap are typically effecive for QA</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074ad9c9-fbbd-47fb-ba9e-e0f0d50d5529",
   "metadata": {},
   "source": [
    "##### <font color = maroon >Why Splitting : There is a limitation respect to context size for exach model </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "97700aa6-2637-4a5a-a18e-486c07bb524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(loader,chunk_size,chunk_overlap,is_separator_regex,strip_whitespace):\n",
    "    \"\"\"\n",
    "    Load and split the text from the file into manageable chunks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = loader.load()\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            is_separator_regex=is_separator_regex,\n",
    "            strip_whitespace = strip_whitespace\n",
    "            \n",
    "        )\n",
    "        return splitter.split_text(str(text))   # split_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading and splitting text from {filename}: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa8d23b-fd5c-4f34-8e8d-89110198a316",
   "metadata": {},
   "source": [
    "#### <font color= green>Loading the file based on file type  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e52e7e1c-c7aa-45f3-a00c-d2b143927a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = load_text(filename,file_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c264deda-53af-41ea-b07e-05b65aab1088",
   "metadata": {},
   "source": [
    "#### Parameters:\n",
    "\n",
    "chunk_size (int) – Maximum size of chunks to return\n",
    "\n",
    "chunk_overlap (int) – Overlap in characters between chunks\n",
    "\n",
    "length_function (Callable[[str], int]) – Function that measures the length of given chunks\n",
    "\n",
    "keep_separator (Union[bool, Literal['start', 'end']]) – Whether to keep the separator and where to place it in each corresponding chunk (True=’start’)\n",
    "\n",
    "add_start_index (bool) – If True, includes chunk’s start index in metadata\n",
    "\n",
    "strip_whitespace (bool) – If True, strips whitespace from the start and end of every document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "14536426-6b3d-4517-b89f-7f350048c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size=100\n",
    "chunk_overlap=20\n",
    "is_separator_regex=False\n",
    "strip_whitespace = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304706d5-a013-4d3a-99ad-d71a0a3f71c5",
   "metadata": {},
   "source": [
    "#### Split the text based on above parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a6f3b11b-55ba-4d1c-8f4a-801f47f56aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = split_text(loader,chunk_size,chunk_overlap,is_separator_regex,strip_whitespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ad8569-acb1-4e79-a742-41c0d2bae215",
   "metadata": {},
   "source": [
    "#### <font color = red> Total number of chunks </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "803718e2-904f-423a-8229-67329a3a218a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "133921e0-0f1a-4181-acbd-a36fa0fb0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(text_chunks):\n",
    "    #print(f'{i}>>>>>>>>{chunk}')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5540c7a9-42da-4a22-b33d-676e2d5ca15a",
   "metadata": {},
   "source": [
    "#### Now Start Embedding using cohere model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab6d672-ee6a-4cf6-a4c7-f9d542ef96bb",
   "metadata": {},
   "source": [
    "## Models and dimentions\n",
    "#####  <b>embed-english-v3.0</b> <font color = green > 1024 </font>\n",
    "\n",
    "#####  embed-multilingual-v3.0 <font color = green > 1024 </font>\n",
    "\n",
    "##### embed-english-light-v3.0 <font color = green > 384 </font>\n",
    "\n",
    "##### embed-multilingual-light-v3.0 <font color = green > 384 </font>\n",
    "\n",
    "##### embed-english-v2.0 <font color = green > 4096 </font>\n",
    "\n",
    "##### embed-english-light-v2.0 <font color = green > 1024 </font> \n",
    "\n",
    "##### embed-multilingual-v2.0 <font color = green > 768 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3539e26-8665-4c5b-b242-2248b99df15c",
   "metadata": {},
   "source": [
    "###### <center><b><font color= blue> why cohere model only </font></b></center> \n",
    "<font color=purple>Cohere models are optimized for \"contextual understanding of language\" , similar to other transformer-based models like BERT or OpenAI’s models. This makes them highly effective for capturing the semantics of a sentence or document, which is crucial for tasks like semantic search, recommendation, or summarization.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "aed5100c-4492-4e1b-8058-867d145a61ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Cohere client\n",
    "def get_cohere_client():\n",
    "    try:\n",
    "        co = cohere.Client(COHERE_API_KEY)\n",
    "        return co\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize Cohere client: {str(e)}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95d7f2f-d062-4e3b-bcd5-987d7b30b306",
   "metadata": {},
   "source": [
    "#### Choosing the Right Input Type\n",
    "For search applications: Use <b>search_query</b> for user input queries and <b>search_document</b> for documents or passages to be searched.<br>\n",
    "For classification tasks: Use classification to ensure the embedding captures characteristics relevant to specific categories or labels.<br>\n",
    "For text generation: Use <b>text_generation</b> if you’re working with tasks that involve creating coherent, flowing text.<br>\n",
    "For summarization: Use summarization if you’re creating a condensed version of a document, aiming to retain key points.<br>\n",
    "\n",
    "#### <font color=green> Each input_type helps to guide the model in producing embeddings best suited for specific types of downstream tasks.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a14eb494-7f33-47a9-9e9c-8cc5583d033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(text):\n",
    "    \"\"\"\n",
    "    Generate embedding using Cohere's embed-english-v3.0 model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        co = get_cohere_client()\n",
    "        response = co.embed(\n",
    "            texts=[text], # accept array \n",
    "            model=\"embed-english-v3.0\",\n",
    "            input_type=\"search_document\" #  focusing on semantic meaning relevant to search contexts, so that they can be compared with \"search_query\" embeddings for efficient and relevant search results.\n",
    "        )\n",
    "        return response.embeddings[0]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in generating embedding: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f52bf31c-c6d8-4324-ba28-51f97eb3a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unique_id(filename):\n",
    "    \"\"\"\n",
    "    Create a unique identifier for the file using the filename.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        base_filename = os.path.basename(filename)\n",
    "        filename_hash = hashlib.md5(base_filename.encode()).hexdigest()\n",
    "        return f\"{base_filename}_{filename_hash[:8]}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create a unique ID for the file: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "444836a0-cfff-47b7-89d8-63a6898fd1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_upsert_embedding(filename, text_chunk, chunk_index):\n",
    "    \"\"\"\n",
    "    Check if embedding exists, if not, create and upsert it.\n",
    "    \"\"\"\n",
    "    file_id = create_unique_id(filename)\n",
    "    print(f\"file_id: {file_id}\")\n",
    "    if not file_id:\n",
    "        print(f\"Failed to create file ID for {filename}\")\n",
    "        return False\n",
    " \n",
    "    unique_id = f\"{file_id}_chunk_{chunk_index}\"\n",
    " \n",
    "    try:\n",
    "        existing_vector = index.fetch(ids=[unique_id]) # check whether text chunk available or not\n",
    " \n",
    "        # If the embedding does not exist, generate and upsert it\n",
    "        if not existing_vector.get('vectors'):\n",
    "            embedding = generate_embedding(text_chunk)\n",
    "            print(\"dimention of the embedding :\",len(embedding))\n",
    "            if embedding:\n",
    "                index.upsert(\n",
    "                    vectors=[(unique_id, embedding, {\"source\": filename, \"text\": text_chunk, \"file_id\": file_id})]\n",
    "                )\n",
    "                print(f\"Generated and upserted new embedding for chunk {chunk_index} from {filename}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Failed to generate embedding for chunk {chunk_index} from {filename}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"Embedding already exists for chunk {chunk_index} from {filename}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {chunk_index} from {filename}: {str(e)}\")\n",
    "        return False\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9a30abf0-0929-4d5c-b781-97e3d6af74d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_chunks(filename, splits):\n",
    "    \"\"\"\n",
    "    Process text chunks and upsert embeddings if required.\n",
    "    \"\"\"\n",
    "    new_splits = []\n",
    "    for i, split in enumerate(splits):\n",
    "        if check_and_upsert_embedding(filename, split, i):\n",
    "            new_splits.append(split)\n",
    " \n",
    "    if new_splits:\n",
    "        print(f\"Generated and upserted new embeddings for {len(new_splits)} new text chunks\")\n",
    "    else:\n",
    "        print(\"No new text chunks required new embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdc452b-8469-4481-8975-aab3f3c4cba9",
   "metadata": {},
   "source": [
    "#### <font color= pink> <center> Vector data base </ceneter> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fc27bebd-66c4-4d19-b64f-335fc09146a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_pinecone():\n",
    "    try:\n",
    "        pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        spec = ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize Pinecone client: {str(e)}\")\n",
    "        sys.exit(1)\n",
    "    return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e7989aa5-2383-46f6-a799-6d9d9d1e9b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_index(pinecone, index_name):\n",
    "    index = pinecone.Index(index_name)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2ed3ece8-4cc0-4c5e-927d-8f1f09d73efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone = init_pinecone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "18dc5329-33e5-404d-aa75-489bb1371c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = connect_to_index(pinecone,INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bab36c9f-5103-4a1f-a538-70e0414b945f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1024,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 18}},\n",
       " 'total_vector_count': 18}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5bf6d1e1-9318-44d1-9dba-037337ce6efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_id: USA.txt_6280614a\n",
      "Embedding already exists for chunk 0 from USA.txt\n",
      "file_id: USA.txt_6280614a\n",
      "Embedding already exists for chunk 1 from USA.txt\n",
      "file_id: USA.txt_6280614a\n",
      "Embedding already exists for chunk 2 from USA.txt\n",
      "file_id: USA.txt_6280614a\n",
      "Embedding already exists for chunk 3 from USA.txt\n",
      "file_id: USA.txt_6280614a\n",
      "Embedding already exists for chunk 4 from USA.txt\n",
      "file_id: USA.txt_6280614a\n",
      "Embedding already exists for chunk 5 from USA.txt\n",
      "file_id: USA.txt_6280614a\n",
      "Embedding already exists for chunk 6 from USA.txt\n",
      "file_id: USA.txt_6280614a\n",
      "Embedding already exists for chunk 7 from USA.txt\n",
      "file_id: USA.txt_6280614a\n",
      "Embedding already exists for chunk 8 from USA.txt\n",
      "file_id: USA.txt_6280614a\n",
      "Embedding already exists for chunk 9 from USA.txt\n",
      "file_id: USA.txt_6280614a\n",
      "Embedding already exists for chunk 10 from USA.txt\n",
      "file_id: USA.txt_6280614a\n",
      "Embedding already exists for chunk 11 from USA.txt\n",
      "file_id: USA.txt_6280614a\n",
      "Embedding already exists for chunk 12 from USA.txt\n",
      "file_id: USA.txt_6280614a\n",
      "Embedding already exists for chunk 13 from USA.txt\n",
      "file_id: USA.txt_6280614a\n",
      "Embedding already exists for chunk 14 from USA.txt\n",
      "file_id: USA.txt_6280614a\n",
      "Embedding already exists for chunk 15 from USA.txt\n",
      "No new text chunks required new embeddings\n"
     ]
    }
   ],
   "source": [
    "process_text_chunks(filename,text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58131c64-1330-4654-a882-db0457187df2",
   "metadata": {},
   "source": [
    "##### <center><font color= orange> CHAT MODEL </font> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6fa75071-435d-4cf9-8397-424ea8124ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.prompts import SystemMessagePromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_cohere import ChatCohere, CohereEmbeddings, CohereRerank\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "import cohere\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6a7d672d-7c1e-4a65-a947-3bd0c75a5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_id=\"dhruv.pdf_1ad54a87\"  #which is given while creating and stroing embeddings\n",
    " \n",
    "# Initialize Cohere embeddings\n",
    "cohere_embeddings = CohereEmbeddings(model=\"embed-english-v3.0\", cohere_api_key=COHERE_API_KEY)\n",
    " \n",
    "# Initialize the language model\n",
    "model_id = \"command-r-plus-08-2024\"\n",
    "llm = ChatCohere(model=model_id, cohere_api_key=COHERE_API_KEY, temperature=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7917c428-a2f2-4a07-874d-a33ea8f42bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the document search with Pinecone vector store\n",
    "docsearch = PineconeVectorStore.from_existing_index(index_name=INDEX_NAME, embedding=cohere_embeddings, text_key=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16069ee0-5043-4995-9d48-7c2cb9166ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0f39c8ef-38ae-42cd-9f78-d6e13a2a39b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(question):\n",
    "    prompt_template = \"\"\"\n",
    "Please provide a short and direct answer from the context below, repeating the exact phrasing of the question in your response.\n",
    "For example, if the question is 'What is the borrower name?', respond with 'Borrower name: [exact answer]'.\n",
    "If the answer cannot be found, respond with 'Apologies, but I couldn't find the information you requested in the document.'.\n",
    "Do not make up any answer.\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    " \n",
    "    messages = [\n",
    "        SystemMessagePromptTemplate.from_template(prompt_template),\n",
    "        HumanMessagePromptTemplate.from_template(question)\n",
    "    ]\n",
    "    prompt = ChatPromptTemplate.from_messages(messages)\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bb915c50-6f35-42c8-93d2-a1c23eb9243f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your question (or type 'exit' to quit):  what is the student name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017817258834838867 retriever time****\n",
      "2.425938367843628 time at chain.invoke\n",
      "Result Student name: Dhruv Vinjamuri\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your question (or type 'exit' to quit):  what is the school name ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17368602752685547 retriever time****\n",
      "2.484807014465332 time at chain.invoke\n",
      "Result Apologies, but I couldn't find the information you requested in the document.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your question (or type 'exit' to quit):  what is the Enrolled Campus?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04800724983215332 retriever time****\n",
      "2.262035846710205 time at chain.invoke\n",
      "Result Enrolled Campus: Early Childhood School\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your question (or type 'exit' to quit):  student id ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019999980926513672 retriever time****\n",
      "2.6309633255004883 time at chain.invoke\n",
      "Result Student ID#: 303265\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your question (or type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_question = input(\n",
    "        \"Please enter your question (or type 'exit' to quit): \"\n",
    "    ).strip()\n",
    " \n",
    "    if user_question.lower() == \"exit\":  # Exit condition\n",
    " \n",
    "        break\n",
    "    # Generate and print the response\n",
    "    prompt = create_prompt(user_question)\n",
    "    chain_type_kwargs = {\"prompt\": prompt}\n",
    "    t1=time.time()\n",
    "    top_k=30\n",
    "    retriever = docsearch.as_retriever(\n",
    "            include_metadata=True,\n",
    "            metadata_key='source',\n",
    "            top_k=30,\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"filter\": {\"file_id\": file_id}}\n",
    "        )\n",
    "    compressor = CohereRerank(model=\"rerank-english-v3.0\",top_n=8)\n",
    "    \n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,\n",
    "        base_retriever=retriever\n",
    "    )\n",
    "    # compressed_docs = compression_retriever.get_relevant_documents(user_question)\n",
    "    # Print the relevant documents from using the embeddings and reranker\n",
    "    # print(compressed_docs)\n",
    "    t2=time.time()\n",
    "    print(t2-t1,\"retriever time****\")\n",
    " \n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,    \n",
    "        chain_type=\"stuff\", \n",
    "        retriever=compression_retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt}\n",
    "    )\n",
    "    \n",
    "    t3=time.time()\n",
    "    answer=chain.invoke({\"query\": user_question})\n",
    "    # print(\"response\",answer)\n",
    "    t4=time.time()\n",
    "    print(t4-t3,\"time at chain.invoke\")\n",
    "    # print(answer)\n",
    "    print(\"Result\",answer[\"result\"])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3258ff4e-a6fb-4d2f-8431-817277953716",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in text_chunks:\n",
    "    embedding = generate_embedding(chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
