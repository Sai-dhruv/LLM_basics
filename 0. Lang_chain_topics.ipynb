{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "458983fb-b5ab-4d40-9c02-01bc2f07797d",
   "metadata": {},
   "source": [
    "#### 1. How to call an LLM (ChatOpenAI, ChatGroq, ChatCohere)\n",
    "\n",
    "        a. Structured text output\n",
    "        b. Streaming responses\n",
    "#### 2. Prompt engineering\n",
    "        a. PromptTemplates\n",
    "        b. ChatPromptTemplate\n",
    "        c. Prompt variables\n",
    "        d. Multi-turn prompts\n",
    "        e. Output Parsers\n",
    "            StrOutputParser\n",
    "            JsonOutputParser\n",
    "            PydanticOutputParser\n",
    "            Tools to enforce structured output\n",
    "#### 3. LLM chaining\n",
    "        LLMChain\n",
    "        SequentialChain\n",
    "        RouterChain\n",
    "        TransformChain\n",
    "        RetrievalChain (v0.1) âœ”ï¸ Correct â€” now part of retrieval module\n",
    "\n",
    "    ðŸ”¹ NEW in LangChain 0.1+\n",
    "        Runnable sequences\n",
    "        Runnable mapping\n",
    "        Parallel chains\n",
    "        Streaming pipelines\n",
    "\n",
    "#### 4. Retrieval-Augmented Generation (RAG)\n",
    "        4.1 Document Loading\n",
    "                * File loaders\n",
    "                    PDF (PyPDFLoader/Docling/Nougat)\n",
    "                    Text\n",
    "                    Word\n",
    "                    HTML\n",
    "                    Markdown\n",
    "                * Web loaders\n",
    "                    WebBaseLoader\n",
    "                    PlaywrightLoader (dynamic sites)\n",
    "                    SitemapLoader\n",
    "                * Cloud loaders\n",
    "                    S3 loader\n",
    "                    GCS loader\n",
    "                    Azure Blob loader\n",
    "        4.2 Text Splitting\n",
    "                    RecursiveCharacterTextSplitter â€” BEST for most cases\n",
    "                    Token-based (OpenAIâ€™s tiktoken)\n",
    "                    Document-specific (Markdown, HTML, Python code)\n",
    "        4.3 Embeddings\n",
    "                * Embeddings convert text â†’ numeric vectors.\n",
    "                * Popular Embedding Models\n",
    "                * OpenAIEmbeddings\n",
    "                * CohereEmbeddings\n",
    "                * HuggingFaceEmbeddings\n",
    "                * Google VertexAI embeddings\n",
    "                * Jina embeddings\n",
    "        4.4 Key Concepts\n",
    "                * Dimensionality                \n",
    "                * Context window\n",
    "                * Similarity metrics (cosine, dot)\n",
    "                * Performance trade-offs\n",
    "\n",
    "        4.5 Vector Stores (Databases for RAG)\n",
    "                FAISS (local)\n",
    "                ChromaDB (local)\n",
    "                Pinecone (managed)\n",
    "                Weaviate\n",
    "                Milvus\n",
    "                Elasticsearch\n",
    "                \n",
    "                Important topics\n",
    "                    Index creation\n",
    "                    Metadata filters\n",
    "                    Hybrid search (dense + keyword)\n",
    "                    Incremental updates\n",
    "        4.6. Retrievers\n",
    "                VectorStoreRetriever\n",
    "                MultiQueryRetriever (uses LLM to rewrite queries)\n",
    "                ContextualCompressionRetriever\n",
    "                ParentDocumentRetriever\n",
    "                EnsembleRetriever (combines BM25 + vector search)\n",
    "\n",
    "        4.7 Retrieval-Augmented Generation (RAG)\n",
    "                The most common RAG pipeline:\n",
    "                Load â†’ Split â†’ Embed â†’ Store â†’ Retrieve â†’ LLM\n",
    "                Advanced RAG techniques\n",
    "                    RAG-Fusion (multiple query expansion)\n",
    "                    LLM-based ranking\n",
    "                    Graph RAG (knowledge graph embeddings)\n",
    "                    HyDE (Hypothetical Document Embeddings)\n",
    "                    Self-RAG / ReAct-RAG\n",
    "\n",
    "                \n",
    "#### 5. Tool usage\n",
    "        Tool definition\n",
    "        Structured arguments\n",
    "        Tool selection\n",
    "        Model routing\n",
    "        Multi-tool orchestration\n",
    "\n",
    "        Example tools\n",
    "            Calculator\n",
    "            Search engine (Bing, Google)\n",
    "            Database query tool\n",
    "            Code execution tool\n",
    "\n",
    "#### 6. Agents\n",
    "    Agents are LLMs that decide what action to take next.\n",
    "    Types of Agents\n",
    "        ReAct Agent (reason + act)\n",
    "        OpenAI Function Agent\n",
    "        Tool-driven agent (multi-tool)\n",
    "        Self-ask agent\n",
    "        Plan-and-execute agent\n",
    "    Agent Loop\n",
    "        LLM thinks\n",
    "        Picks a tool\n",
    "        Tool executes\n",
    "        LLM sees result\n",
    "        Decides next step\n",
    "\n",
    "#### 7. Memory\n",
    "\n",
    "Memory enables persistent conversation.\n",
    "\n",
    "        BufferMemory\n",
    "        BufferWindowMemory\n",
    "        ConversationSummaryMemory\n",
    "        ConversationSummaryBufferMemory\n",
    "        VectorStoreMemory\n",
    "        \n",
    "        Production-ready memory notes\n",
    "            Avoid infinite memory\n",
    "            Use summarization\n",
    "            Use external storage (Redis, DynamoDB, Postgres)\n",
    "\n",
    "\n",
    "\n",
    "#### 7. Workflow orchestration\n",
    "\n",
    "    LangGraph (most important new module)\n",
    "        Pregel-style workflow graphs\n",
    "        Multi-step workflows\n",
    "        Agent workflows\n",
    "        State management\n",
    "        Checkpoints, retries, streaming\n",
    "\n",
    "    LangGraph is now the industry standard for building:\n",
    "\n",
    "        Multi-agent systems\n",
    "        RAG pipelines\n",
    "        Complex workflows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c44bd2e-7050-4944-897e-c24413aff322",
   "metadata": {},
   "source": [
    "### âœ… Call an OpenAI Chat Model (LangChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133da4a5-888f-4c0f-9ee3-1f8d3fa6f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o-mini\",\n",
    "                 api_key = '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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbf2bf2-bad6-4019-92dd-89eabeea6951",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(\"Explain Generative AI in one paragraph.\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dd79f6-32c8-482e-b372-5fe302bc1088",
   "metadata": {},
   "source": [
    "### âœ… Structured Output (Text Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801718b9-e7d8-464f-bfa0-69d6122abce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(\n",
    "    \"List 3 benefits of LangChain in bullet points.\"\n",
    ")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476d2317-8bba-4d92-8922-1d44a22462ae",
   "metadata": {},
   "source": [
    "### âœ… Streaming Response from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fbe974-3e90-4e70-9b75-df1c87f0d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in llm.stream(\"Write a poem about data engineering.\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbb044e-977f-466c-a956-562b1e0553ca",
   "metadata": {},
   "source": [
    "## ðŸ”¥ 2.2 Prompts\n",
    "### âœ… PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d839ae-3a6a-4eb6-93e1-025b3221224e",
   "metadata": {},
   "source": [
    "These are reusable templates where you define placeholders for dynamic values. They help structure and standardize prompts so you donâ€™t manually rewrite them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c743b2a-76b1-48db-a982-557f72c0e3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain the importance of {topic} in simple words.\"\n",
    ")\n",
    "\n",
    "prompt = template.format(topic=\"feature engineering\")\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f634fd95-5c34-45c0-a460-a676235b424c",
   "metadata": {},
   "source": [
    "### âœ… ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16feac5-b2bf-4e2a-8460-c888b7fd5081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert software engineer.\"),\n",
    "    (\"user\", \"Explain {concept} with an example.\")\n",
    "])\n",
    "\n",
    "final_prompt = prompt.format(concept=\"retrieval augmented generation\")\n",
    "print(final_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b711ef-7c6a-4133-8a83-751351185069",
   "metadata": {},
   "source": [
    "### âœ… Using Prompt Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b9b816-bdbb-4fe2-a6de-863a6a27f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a {length} story about {character}.\",\n",
    "    input_variables=[\"length\", \"character\"]\n",
    ")\n",
    "\n",
    "print(prompt.format(length=\"short\", character=\"a robot\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72000a08-9ee7-4532-9557-f4e908ddd91f",
   "metadata": {},
   "source": [
    "### âœ… Multi-turn Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f07dd9-482b-4af1-9f0b-39185fa4439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"What is LangChain?\"),\n",
    "    (\"assistant\", \"LangChain is an LLM framework.\"),\n",
    "    (\"user\", \"Why is it useful?\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf9cccb-ab7b-4c08-86c7-e608b1aa29ee",
   "metadata": {},
   "source": [
    "### ðŸ”¥ 2.3 Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b814f06-c9b3-4301-baa9-31b8c77baec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = llm | parser\n",
    "\n",
    "print(chain.invoke(\"Explain embeddings in one sentence.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24e12b7-2e36-4491-b69d-1c861c496852",
   "metadata": {},
   "source": [
    "### âœ… Json Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61edc6a-a16b-4fa0-a5d7-4dd29f040072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Provide the output in JSON format:\n",
    "{{\n",
    "   \"topic\": \"...\",\n",
    "   \"summary\": \"...\"\n",
    "}}\n",
    "Topic: {topic}\n",
    "\"\"\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "chain = template | llm | parser\n",
    "\n",
    "result = chain.invoke({\"topic\": \"Transformers\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58048b9-1273-4800-806c-d0f4dd85fe14",
   "metadata": {},
   "source": [
    "### âœ… Pydantic Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ca73a1-fac1-4245-9120-bd901d8d3d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "class BookInfo(BaseModel):\n",
    "    title: str\n",
    "    author: str\n",
    "    genre: str\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=BookInfo)\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template=\"Generate book info.\\n{format_instructions}\",\n",
    "    input_variables=[],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = template | llm | parser\n",
    "result = chain.invoke({})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84803b66-a769-4af3-b077-2efae0ed1242",
   "metadata": {},
   "source": [
    "### ðŸ”¥ Tools to Enforce Structured Output (LangChain Expression Language)\n",
    "##### Example: Forcing JSON Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a753cfc0-97d0-45bf-af60-a8e52b941764",
   "metadata": {},
   "outputs": [],
   "source": [
    "forced_json_prompt = \"\"\"\n",
    "You MUST return valid JSON in this format:\n",
    "{\n",
    "  \"name\": \"\",\n",
    "  \"age\": \"\",\n",
    "  \"skills\": []\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "response = llm.invoke(forced_json_prompt)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbface7-da0e-42d4-b711-c1036a15c548",
   "metadata": {},
   "source": [
    "### â­ Bonus: OpenAI + LangChain Conversational Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb6bcf2-8323-4add-b177-8915865dc917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(chain.invoke({\"question\": \"Explain LLMs in simple words\"}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77f95a-fa5f-4669-9c90-a28189a58104",
   "metadata": {},
   "source": [
    "### ðŸ”¥ 3. CHAINS (LangChain Basics)\n",
    "\n",
    "Chains = connecting multiple components into a pipeline:\n",
    "\n",
    "Prompt â†’ LLM â†’ Output Parser\n",
    "\n",
    "Loader â†’ Splitter â†’ VectorStore â†’ Retriever â†’ LLM\n",
    "\n",
    "Decision routing\n",
    "\n",
    "Multi-step logic\n",
    "\n",
    "Below are examples of ALL important chain types:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c13d8d-a262-4e02-998a-073b111f760c",
   "metadata": {},
   "source": [
    "### âœ… 3.1 LLMChain â€” Basic Prompt â†’ LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0739147-3ba0-4cf3-b06e-6bf67129776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in one paragraph.\",\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "print(chain.invoke({\"topic\": \"feature engineering\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8313d-a4b1-406f-83dd-b8e7f39affbf",
   "metadata": {},
   "source": [
    "### âœ… 3.2 SequentialChain â€” Multi-step pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d0954-180f-487c-b386-eccfdc0a12d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Step 1: Generate ideas\n",
    "idea_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Generate 3 creative ideas about {topic}.\"\n",
    ")\n",
    "\n",
    "# Step 2: Expand the best idea\n",
    "expand_prompt = PromptTemplate(\n",
    "    input_variables=[\"ideas\"],\n",
    "    template=\"Pick the best idea from this list and expand it into a paragraph:\\n{ideas}\"\n",
    ")\n",
    "\n",
    "# --- Build the pipeline ---\n",
    "\n",
    "# Step 1 â†’ produce \"ideas\"\n",
    "step1 = idea_prompt | llm | parser\n",
    "\n",
    "# Step 2 â†’ consume \"ideas\" â†’ produce final output\n",
    "step2 = expand_prompt | llm | parser\n",
    "\n",
    "# Complete chain\n",
    "chain = (\n",
    "    {\"ideas\": step1}  # Run step1 and store output in \"ideas\"\n",
    "    | step2           # Pass \"ideas\" into step2\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"topic\": \"AI in healthcare\"})\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25846c93-d725-4977-a5b6-60f9537a2f11",
   "metadata": {},
   "source": [
    "### â­ 3.3 RouterChain â€” Choose the right chain based on input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ffbb54-be3a-4272-895e-21bcdbed2d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Destination prompts\n",
    "math_prompt = PromptTemplate.from_template(\"Solve this math problem: {input}\")\n",
    "joke_prompt = PromptTemplate.from_template(\"Tell a joke about: {input}\")\n",
    "define_prompt = PromptTemplate.from_template(\"Provide a definition of: {input}\")\n",
    "\n",
    "# Default fallback prompt\n",
    "default_prompt = PromptTemplate.from_template(\"Answer this question normally: {input}\")\n",
    "\n",
    "# Router logic (replacement for MultiPromptChain)\n",
    "router = RunnableBranch(\n",
    "    # If input looks like math\n",
    "    (lambda x: any(c.isdigit() for c in x[\"input\"]), math_prompt | llm | parser),\n",
    "\n",
    "    # If input mentions \"joke\"\n",
    "    (lambda x: \"joke\" in x[\"input\"].lower() or \"funny\" in x[\"input\"].lower(),\n",
    "        joke_prompt | llm | parser),\n",
    "\n",
    "    # If input asks for definition\n",
    "    (lambda x: \"define\" in x[\"input\"].lower() or \"what is\" in x[\"input\"].lower(),\n",
    "        define_prompt | llm | parser),\n",
    "\n",
    "    # Default fallback\n",
    "    default_prompt | llm | parser\n",
    ")\n",
    "\n",
    "# Test\n",
    "print(router.invoke({\"input\": \"5 + 7\"}))\n",
    "print(router.invoke({\"input\": \"Tell me something funny\"}))\n",
    "print(router.invoke({\"input\": \"Explain transformers\"}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6650f483-b9ec-4564-993e-2920edcb9675",
   "metadata": {},
   "source": [
    "### â­ TransformChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1a442b-b38f-4fd1-a087-bd2d4356d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Step 1: Clean the text\n",
    "clean_step = RunnableLambda(lambda x: {\"text\": x[\"text\"].strip()})\n",
    "\n",
    "# Step 2: Format into a prompt (REQUIRED!)\n",
    "prompt = PromptTemplate.from_template(\"Answer this: {text}\")\n",
    "\n",
    "# Step 3: Final chain\n",
    "chain = clean_step | prompt | llm | parser\n",
    "\n",
    "print(chain.invoke({\"text\": \"   Explain feature engineering   \"}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7133220e-999f-49bc-8d97-5186d95eb843",
   "metadata": {},
   "source": [
    "### ðŸ”µ 2. RetrievalChain â€” Auto RAG Pipeline\n",
    "\n",
    "A RetrievalChain combines:\n",
    "\n",
    "Retriever\n",
    "\n",
    "Prompt\n",
    "\n",
    "LLM\n",
    "\n",
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2d5abb-b0d7-4507-a0cb-ae53f8a1ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_community\n",
    "#!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11ef1854-5814-4a71-8ad3-8e8e60dd68f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f4765b1-0442-43ff-973c-301a47157291",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: 1471e942************************0f29. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m docs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLangChain is a framework for building LLM apps.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLangGraph is used for multi-agent workflows.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAG stands for Retrieval Augmented Generation.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m ]\n\u001b[0;32m      7\u001b[0m emb \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-embedding-3-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m retriever \u001b[38;5;241m=\u001b[39m vectorstore\u001b[38;5;241m.\u001b[39mas_retriever()\n",
      "File \u001b[1;32mP:\\GEN-AI\\langchain-application\\.venv\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1043\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[0;32m   1018\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1024\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m \n\u001b[0;32m   1027\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1043\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[0;32m   1045\u001b[0m         texts,\n\u001b[0;32m   1046\u001b[0m         embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1051\u001b[0m     )\n",
      "File \u001b[1;32mP:\\GEN-AI\\langchain-application\\.venv\\lib\\site-packages\\langchain_openai\\embeddings\\base.py:588\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m    587\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[1;32m--> 588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mP:\\GEN-AI\\langchain-application\\.venv\\lib\\site-packages\\langchain_openai\\embeddings\\base.py:483\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    481\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[1;32m--> 483\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    484\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mtokens[i : i \u001b[38;5;241m+\u001b[39m _chunk_size], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invocation_params\n\u001b[0;32m    485\u001b[0m     )\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    487\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[1;32mP:\\GEN-AI\\langchain-application\\.venv\\lib\\site-packages\\openai\\resources\\embeddings.py:124\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    118\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    119\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mP:\\GEN-AI\\langchain-application\\.venv\\lib\\site-packages\\openai\\_base_client.py:1278\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1266\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1273\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1274\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1275\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1276\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1277\u001b[0m     )\n\u001b[1;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mP:\\GEN-AI\\langchain-application\\.venv\\lib\\site-packages\\openai\\_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    953\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mP:\\GEN-AI\\langchain-application\\.venv\\lib\\site-packages\\openai\\_base_client.py:1059\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1056\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1058\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1059\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1062\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1063\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1067\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1068\u001b[0m )\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: 1471e942************************0f29. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "    \"LangChain is a framework for building LLM apps.\",\n",
    "    \"LangGraph is used for multi-agent workflows.\",\n",
    "    \"RAG stands for Retrieval Augmented Generation.\",\n",
    "]\n",
    "\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = FAISS.from_texts(docs, embedding=emb)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84815ce-e0ed-4212-bb35-34b6efe21f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
